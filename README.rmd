# NakedMoleRat :smile: :wolf:

## :rocket: Here is what you need to do to utilize these particular R scripts!
1. Download the .Rproj file and the sample data file 
2. Open the project file in R studio
3. Set the working directory in R studio to the one with the data file 
4. Edit the script accordingly with the configuration information

## Some notes: 

Hello! I am currently taking part in research utilizing Naked Mole Rats. I am utilizing data analysis techniques to answer the overall question, "Can we detect certain types of naked mole rat behavior utilizing  statistical techniques such as hamming distance and similarity graphs". 

Certain statistical techniques, such as hamming distance have been utilized to detect sleeping patterns, however, that is a pretty blatant type of behavior. Whereas, analyzing particular behavior after the birth of a new pup is much harder to detect. 

My main programming language is Java and I am new to R. I have written two scripts that are particularly important to the research. The first one, which I will post here, reads in a state matrix with 59 columns and an undetermined amount of rows per file. 

Each column signifies a rat, each row signifies one time window, and each data point represents the location the animal is found at  

One row of the data looks like this written on one line (an example of a file can be found [here](https://drive.google.com/file/d/0B09uYnicWpbLY25SUmNpTzFwS0k/view?usp=sharing):

2 6 3 1 5 2 4 5 7 1 1 1 1 1 2 1 2 1 2 3 1 2 2 2 1 1 2 1 1 3 2 2 2 1 2 2 1 1 2 3 7 2 2 2 2 2 1 2 1 2 2 1 1 2 2 4 1 3 4

The program below takes in this matrix, creates an adjacency matrix for the specific file (ie a matrix 59 x 59) that counts how many time windows rat 1 has been with rat 2, and how many time windows rat 1 has been with rat 3, etc. Additionally, the program then creates a thresholded matrix to see which animals spend more then, for example, 50% percent of their time together. 

    #This version of code works with multiple data files in a particular folder, 
    #performs some matrix calculations on them and outputs the new matrices with their corresponding titles 
    #into that same folder - currently there are 165 stateFiles within this particular folder which 
    #means 330 additional files were created in that same folder in about a minutes worth of time. 
    
    #Asides: 
    #Need to maintain and edit code to follow R style consistently (ie stop using = and <- interchangably) 
    #Declare a nrow(df) var and an ncol(df) column var 

    stateFiles <- list.files("/home/zackymo/Desktop/birth_data_zach_calcs_75/", pattern="*state.txt", full.names=TRUE)
    thres <- .75

    for(file in stateFiles){
      rawData = read.table(file)
      df <- data.frame(rawData)
      incidence <- matrix(rep(0, ncol(df)*ncol(df)), nrow=ncol(df))
      thresholded <- matrix(rep(0, ncol(df)*ncol(df)), nrow=ncol(df)) 
  
      #Set the diagonal = to constants due to the fact that both are symmetric matrices 
      diag(incidence) <- nrow(df)
      diag(thresholded) <- 1
  
      #These loops turn the state matrix into a boolean matrix, and then counts up the true and falses, adds them to the incidence matrix, and then creates the thresholded matrix. 
      for (i in 1:(ncol(df)-1)) {
        for (j in (i+1):ncol(df)) {
          incidence[i,j] = incidence[j,i] = sum(df[,i] == df[,j])
          if(incidence[i,j]/nrow(df) >= thres){
            thresholded[i,j] = thresholded[j,i] = 1
          }#should these conditions be reversed for effiency? How can I check that? 
          else{
            thresholded[i,j] = thresholded[j,i] = 0
          }
        }
      }
      #Data preparation for output file - transposing matrices - recommended by other R users for write function,
      #should get rid of in future to simplify code
      tincidence <- t(incidence) 
      tthresholded <- t(thresholded)
  
      #Manipulating the fileName to be able to create a new file w/ a meaningful file name
      #I may be able to split by / then edit the paste below f[1] + f[2] +   f[3] + f[4] + f[5]
      f <- strsplit(file, split='.', fixed=TRUE)
      f <- unlist(f)
  
      #Creating the various output files 
     write(tincidence, file = paste(f[1],"toInc", sep = ""),
            ncolumns = ncol(df),
            sep = " ") 
  
      write(tthresholded, file = paste(f[1],"toThresholded", sep = ""),
            ncolumns = ncol(df),
            sep = " ") 
    }
    
The second program utilizes the hamming distance metric and compares the first newly created threshold matrix file (all animal behavior throughout the course of a day) of consistent activity against all other files in order to establish how the animals behavior differed throughout the day. 

    setwd("/home/zackymo/Desktop/NakedMoleRat/birth_data_zach_calcs_75_V1/")
    threshFiles <- list.files(getwd(), pattern="*Thresholded", full.names=TRUE)
    folderLength <- length(threshFiles)
    hammingCount <- vector(mode="numeric", length=foldLength)
    
    firstThresh <- read.table(threshFiles[1])
    firstDf <- data.frame(firstThresh)
    
    for (i in 2:folderLength){
      count <- 0
      iThresh <- read.table(threshFiles[i])
      iDf <- data.frame(iThresh)
      for (k in 1:(ncol(firstDf)-1)) {
        for (l in (k+1):ncol(firstDf)) {
          if (abs(firstDf[k,l] - iDf[k,l]) == 1){
            count <- count + 1
          } 
        }
      }
      hammingCount[i] <- count
    }

    write.csv(hammingCount, file = "hammingCountThres75.csv") 
    
I have added an excel file with the initial findings. There is approximately 7 full days of data on this graph. For simplicity, you can see the image [here](https://drive.google.com/file/d/0B09uYnicWpbLeGtfbDRSOVNZbUE/view?usp=sharing):exclamation::squirrel:

--------------------------------------------------------------------------------------------------

Updated portion of README -

I have created a script in python that runs through all of the data files accessible to me 

The data is still like the sample one in the data directory, however, I have received more of it. 

Summary of the scripts abilities thus far:

In 12 mins, it ran through 800 files (500 mb worth of data), and created approximately 1600 files, while only using 30 % of my cpu capacity

This can be enhanced incredibly by adding using python nb clusters I beleive, and also by limiting the amount of times I open and write to files, which in this brute force method is currently a lot. 

I will try to do this further down the line (within the next two weeks or so) after conducting the mode analysis, along with some introductory social network analysis. 